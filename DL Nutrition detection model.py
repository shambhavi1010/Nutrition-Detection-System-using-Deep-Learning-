# -*- coding: utf-8 -*-
"""Copy of FOOD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AnpjePoxijkJ-qHAuGVhZJ_ilnyb-wbW

##DATASET
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Assuming you have a CSV file with columns 'file_path' and 'label'
data = pd.read_csv('/content/drive/MyDrive/Food Images/image_labels.csv')
file_paths = data['Image_Path'].tolist()
labels = data['Label'].tolist()

data

"""##VISUALIZATION"""

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming you have 'labels' containing the category labels for each image
sns.set(style="whitegrid")
plt.figure(figsize=(12, 6))
sns.countplot(x=labels)
plt.title("Distribution of Food Categories")
plt.xlabel("Category")
plt.ylabel("Count")
plt.xticks(rotation=90)
plt.show()

# Plot a bar chart to visualize the class distribution
plt.figure(figsize=(10, 6))
data['label'].value_counts().plot(kind='bar')
plt.title('Class Distribution in the Dataset')
plt.xlabel('Class')
plt.ylabel('Count')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.show()

import random
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# Define the path to your dataset directory
dataset_dir = 'food_dataset'  # Replace with the path to your dataset

# Number of sample images to display
num_samples = 9

# Create a subplot with sample images
plt.figure(figsize=(12, 8))
for i in range(num_samples):
    # Select a random image from the dataset
    random_image_path = random.choice(file_paths)

    # Read and display the image
    img = mpimg.imread(random_image_path)
    plt.subplot(3, 3, i + 1)
    plt.imshow(img)
    plt.axis('off')

plt.suptitle('Sample Images from the Dataset')
plt.show()

# Create a pie chart to visualize class proportions
plt.figure(figsize=(8, 8))
class_counts = data['label'].value_counts()
plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', startangle=140)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('Class Proportions in the Dataset')
plt.show()

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Function to compute histograms for each color channel
def compute_color_histogram(image):
    red_hist = cv2.calcHist([image], [0], None, [256], [0, 256])
    green_hist = cv2.calcHist([image], [1], None, [256], [0, 256])
    blue_hist = cv2.calcHist([image], [2], None, [256], [0, 256])
    return red_hist, green_hist, blue_hist

# Select a random image from the dataset
random_image_path = random.choice(file_paths)
image = cv2.imread(random_image_path)

# Compute color histograms
red_hist, green_hist, blue_hist = compute_color_histogram(image)

# Plot histograms
plt.figure(figsize=(12, 4))
plt.subplot(131)
plt.plot(red_hist, color='red')
plt.title('Red Channel Histogram')
plt.subplot(132)
plt.plot(green_hist, color='green')
plt.title('Green Channel Histogram')
plt.subplot(133)
plt.plot(blue_hist, color='blue')
plt.title('Blue Channel Histogram')
plt.show()

"""##ENCODING"""

# Step 1: Convert labels to numerical values
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(labels)

# Step 2: Apply one-hot encoding
onehot_encoder = OneHotEncoder(sparse=False)
encoded_labels = encoded_labels.reshape(-1, 1)  # Reshape to a column vector
onehot_labels = onehot_encoder.fit_transform(encoded_labels)

# Create a DataFrame with file paths and one-hot encoded labels
encoded_data = pd.DataFrame({'file_path': file_paths})
encoded_data = pd.concat([encoded_data, pd.DataFrame(onehot_labels, columns=label_encoder.classes_)], axis=1)

# Display the first few rows of the encoded dataset
print(encoded_data.head())

# Optionally, save the encoded dataset to a new CSV file
encoded_data.to_csv('encoded_dataset.csv', index=False)

"""##PREPROCESSING AND SPLITTING"""

import os
import random
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score

# Set random seed for reproducibility
random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

# Define dataset directory and parameters
dataset_dir = '/content/drive/MyDrive/Food Images'  # Replace with the path to your dataset
img_size = (224, 224)
batch_size = 32
epochs = 40

# Create a DataFrame to store file paths and labels
file_paths = []
labels = []

# Loop through subdirectories in the dataset directory
for class_name in os.listdir(dataset_dir):
    class_dir = os.path.join(dataset_dir, class_name)
    if os.path.isdir(class_dir):
        for image_filename in os.listdir(class_dir):
            if image_filename.endswith(('.jpg', '.jpeg', '.png')):
                file_paths.append(os.path.join(class_dir, image_filename))
                labels.append(class_name)

# Create a DataFrame from the file paths and labels
data = pd.DataFrame({'file_path': file_paths, 'label': labels})

# Encode labels with one-hot encoding
label_encoder = LabelEncoder()
data['encoded_label'] = label_encoder.fit_transform(data['label'])
num_classes = len(label_encoder.classes_)

# Split the dataset into train, validation, and test sets
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)
valid_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)

# Image data augmentation for training set
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Image data augmentation for validation and test sets (only rescaling)
valid_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

# Create data generators
train_generator = train_datagen.flow_from_dataframe(
    dataframe=train_data,
    x_col='file_path',
    y_col='encoded_label',
    target_size=img_size,
    batch_size=batch_size,
    class_mode='raw'
)

valid_generator = valid_datagen.flow_from_dataframe(
    dataframe=valid_data,
    x_col='file_path',
    y_col='encoded_label',
    target_size=img_size,
    batch_size=batch_size,
    class_mode='raw'
)

test_generator = test_datagen.flow_from_dataframe(
    dataframe=test_data,
    x_col='file_path',
    y_col='encoded_label',
    target_size=img_size,
    batch_size=batch_size,
    class_mode='raw'
)

"""##CNN MODEL"""

# Create a CNN model
model_1 = Sequential()
model_1.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)))
model_1.add(MaxPooling2D(2, 2))
model_1.add(Conv2D(64, (3, 3), activation='relu'))
model_1.add(MaxPooling2D(2, 2))
model_1.add(Conv2D(128, (3, 3), activation='relu'))
model_1.add(MaxPooling2D(2, 2))
model_1.add(Flatten())
model_1.add(Dense(128, activation='relu'))
model_1.add(Dropout(0.5))
model_1.add(Dense(num_classes, activation='softmax'))

# Compile the model
model_1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model_1.summary()

# Train the model
history = model_1.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=epochs,
    validation_data=valid_generator,
    validation_steps=len(valid_generator),
    shuffle=True
)

# Evaluate the model on the test set
test_loss, test_accuracy = model_1.evaluate(test_generator, steps=len(test_generator))
print(f'Test accuracy: {test_accuracy * 100:.2f}%')

# Optionally, save the trained model

model_1.save('CNN_MODEL.h5')

from sklearn.metrics import classification_report

# Assuming you have already trained your model and loaded the test_data DataFrame
true_labels = [label_encoder.transform([label])[0] for label in test_data['label']]

# Make predictions using the model
y_pred_probs = model_1.predict(test_generator)
y_pred = np.argmax(y_pred_probs, axis=1)

from sklearn.metrics import classification_report

# Generate the classification report
report = classification_report(true_labels, y_pred, target_names=label_encoder.classes_)

# Print the classification report
print(report)

"""##CNN MODEL 2"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, AveragePooling2D, Dropout

# Create a Sequential model
model_avg = Sequential()
# Create a CNN model
# Convolutional and Pooling Layers
model_avg.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))
model_avg.add(MaxPooling2D(2, 2))

model_avg.add(Conv2D(64, (3, 3), activation='relu'))
model_avg.add(MaxPooling2D(2, 2))

model_avg.add(Conv2D(128, (3, 3), activation='relu'))
model_avg.add(AveragePooling2D(2, 2))

model_avg.add(Conv2D(256, (3, 3), activation='relu'))
model_avg.add(AveragePooling2D(2, 2))
# Flatten layer
model_avg.add(Flatten())

# Fully Connected Layers
model_avg.add(Dense(512, activation='relu'))
model_avg.add(Dropout(0.5))

model_avg.add(Dense(256, activation='relu'))
model_avg.add(Dropout(0.5))

# Output layer
model_avg.add(Dense(num_classes, activation='softmax'))

# Compile the model
model_avg.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model_avg.summary()

# Train the model
history = model_avg.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=epochs,
    validation_data=valid_generator,
    validation_steps=len(valid_generator),
    shuffle=True
)

# Evaluate the model on the test set
test_loss, test_accuracy = model_avg.evaluate(test_generator, steps=len(test_generator))
print(f'Test accuracy: {test_accuracy * 100:.2f}%')

# Optionally, save the trained model
model_avg.save('food_classification_model.h5')

model_avg.save('CNN_MODEL_avg.h5')

from sklearn.metrics import classification_report ,confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns


# Assuming you have already trained your model and loaded the test_data DataFrame
true_labels = [label_encoder.transform([label])[0] for label in test_data['label']]
# Make predictions using the model
y_pred_probs = model_avg.predict(test_generator)
y_pred = np.argmax(y_pred_probs, axis=1)


# Create a confusion matrix
conf_matrix = confusion_matrix(true_labels, y_pred)

# Generate a classification report
class_report = classification_report(true_labels, y_pred, target_names=label_encoder.classes_)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Display the classification report
print("Classification Report:")
print(class_report)

from sklearn.metrics import classification_report

# Generate the classification report
report = classification_report(true_labels, y_pred, target_names=label_encoder.classes_)

# Print the classification report
print(report)

"""## CNN 3"""

# Create a Sequential model
model = Sequential()

# Convolutional layers
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(MaxPooling2D(2, 2))

model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(2, 2))

model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(2, 2))

model.add(Conv2D(256, (3, 3), activation='relu'))
model.add(MaxPooling2D(2, 2))

# Flatten the output
model.add(Flatten())

# Fully connected layers
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))

# Output layer with softmax activation for classification
model.add(Dense(num_classes, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Display model summary
model.summary()

# Train the model
history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=epochs,
    validation_data=valid_generator,
    validation_steps=len(valid_generator),
    shuffle=True
)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(test_generator, steps=len(test_generator))
print(f'Test accuracy: {test_accuracy * 100:.2f}%')

# Optionally, save the trained model
model.save('food_classification_model.h5')

model.save('CNN_MODEL3.h5')

"""##CNN WITH PSO"""

pip install pyswarm

import numpy as np
from pyswarm import pso
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
from sklearn.metrics import accuracy_score

# Define an objective function that trains and evaluates a CNN with given hyperparameters
def objective_function(hyperparameters):
    learning_rate, num_filters = hyperparameters

    # Define and compile the CNN architecture
    model = models.Sequential()
    model.add(layers.Conv2D(num_filters, (3, 3), activation='relu', input_shape=(32, 32, 3)))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Flatten())
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(10, activation='softmax'))

    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # Train the model
    model.fit(train_data,train_labels, epochs=5, verbose=2)

    # Evaluate the model on the validation set
    y_pred = model.predict(test_data)
    y_pred = np.argmax(y_pred, axis=1)
    accuracy = accuracy_score(test_labels, y_pred)

    # Return the negative accuracy as PSO minimizes the objective function
    return -accuracy

# Define the search space (bounds) for hyperparameters
lb = [0.0001, 16]  # Lower bounds for learning rate and number of filters
ub = [0.01, 128]    # Upper bounds for learning rate and number of filters

# Perform PSO optimization
best_hyperparameters, _ = pso(objective_function, lb, ub, swarmsize=10, maxiter=10)

# Print the best hyperparameters found
learning_rate, num_filters = best_hyperparameters
print(f"Best Learning Rate: {learning_rate}")
print(f"Best Number of Filters: {num_filters}")

# Train the CNN with the best hyperparameters
best_model = models.Sequential()
best_model.add(layers.Conv2D(int(num_filters), (3, 3), activation='relu', input_shape=(32, 32, 3)))
best_model.add(layers.MaxPooling2D((2, 2)))
best_model.add(layers.Flatten())
best_model.add(layers.Dense(64, activation='relu'))
best_model.add(layers.Dense(10, activation='softmax'))

optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
best_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
best_model.fit(train_data,train_labels, epochs=10, verbose=2)

# Evaluate the trained model
test_loss, test_acc = best_model.evaluate(test_data, test_labels, verbose=2)
print(f"Test Accuracy with Best Hyperparameters: {test_acc}")





"""##VGG MODEL"""

from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Sequential, Model

# Load the pre-trained VGG16 model without top layers (include_top=False)
vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the pre-trained layers
for layer in vgg_base.layers:
    layer.trainable = False

# Build a custom top classifier for the VGG16 base
x = vgg_base.output
x = Flatten()(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(num_classes, activation='softmax')(x)

# Create the combined VGG16 model
model_vgg = Model(inputs=vgg_base.input, outputs=predictions)

# Compile the model
model_vgg.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model_vgg.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=20,
    validation_data=valid_generator,
    validation_steps=len(valid_generator),
    shuffle=True
)

# Evaluate the model on the test set
test_loss, test_accuracy = model_vgg.evaluate(test_generator, steps=len(test_generator))
print(f'Test accuracy: {test_accuracy * 100:.2f}%')

# Save the model to a file
model_vgg.save('vgg16_food_classification_model.h5')

from sklearn.metrics import classification_report

# Generate predictions on the test set using the model
y_pred = model_vgg.predict(test_generator)

# Convert predicted probabilities to class labels
predicted_labels = np.argmax(y_pred, axis=1)

# Get true labels from the test data DataFrame
true_labels = test_data['encoded_label']

# Get class labels
class_labels = list(label_encoder.classes_)

# Generate the classification report
report = classification_report(true_labels, predicted_labels, target_names=class_labels)

# Print the classification report
print(report)



"""##RESNET"""

import os
import random
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))

model = Sequential()
model.add(resnet_base)
model.add(GlobalAveragePooling2D())
model.add(Dense(128, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.summary()

history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=20,
    validation_data=valid_generator,
    validation_steps=len(valid_generator)
)

"""##ALEXNET"""

# Create an AlexNet model
model = Sequential()

# Convolutional Layer 1
model.add(Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=(img_size[0], img_size[1], 3)))
model.add(MaxPooling2D((3, 3), strides=(2, 2)))

# Convolutional Layer 2
model.add(Conv2D(256, (5, 5), activation='relu'))
model.add(MaxPooling2D((3, 3), strides=(2, 2)))

# Convolutional Layer 3
model.add(Conv2D(384, (3, 3), activation='relu'))

# Convolutional Layer 4
model.add(Conv2D(384, (3, 3), activation='relu'))

# Convolutional Layer 5
model.add(Conv2D(256, (3, 3), activation='relu'))
model.add(MaxPooling2D((3, 3), strides=(2, 2)))

# Flatten the output and add fully connected layers
model.add(Flatten())
model.add(Dense(4096, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(4096, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=epochs,
    validation_data=valid_generator,
    validation_steps=len(valid_generator)
)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(test_generator, steps=len(test_generator))
print(f'Test accuracy: {test_accuracy * 100:.2f}%')

from sklearn.metrics import classification_report
# Calculate and print the classification report
y_pred = model.predict(test_generator)
predicted_labels = np.argmax(y_pred, axis=1)
true_labels = test_data['encoded_label']
class_labels = list(label_encoder.classes_)
report = classification_report(true_labels, predicted_labels, target_names=class_labels)
print(report)

# Optionally, save the trained model
model.save('alexnet_food_classification_model.h5')





"""##MOBILENET"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score

base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(128, activation='relu'),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.summary()

history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=epochs,
    validation_data=valid_generator,
    validation_steps=len(valid_generator)
)

test_loss, test_accuracy = model.evaluate(test_generator, steps=len(test_generator))
print(f'Test accuracy: {test_accuracy * 100:.2f}%')

model.save('Mobilenet.h5')

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming you have already trained your model and loaded the test_data DataFrame
true_labels = [label_encoder.transform([label])[0] for label in test_data['label']]

# Make predictions using the MobileNet model
y_pred_probs = model.predict(test_generator)
y_pred = np.argmax(y_pred_probs, axis=1)

# Create a confusion matrix
conf_matrix = confusion_matrix(true_labels, y_pred)

# Generate a classification report
class_report = classification_report(true_labels, y_pred, target_names=label_encoder.classes_)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Display the classification report
print("Classification Report:")
print(class_report)

from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Get the true labels and predictions
true_labels = test_generator.classes
predicted_labels = model.predict(test_generator).argmax(axis=1)

# Calculate the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Display a classification report
report = classification_report(true_labels, predicted_labels, target_names=label_encoder.classes_)
print(report)

"""##DEPLOYMENT"""

pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# # app.py
# %%writefile app.py
# import streamlit as st
# import pickle
# import tensorflow as tf
# from PIL import Image
# import numpy as np
# 
# # Load your VGG model
# model = tf.keras.models.load_model('/content/vgg16_food_classification_model.keras')
# 
# # Load your lookup table using Pickle
# with open('/content/lookup_table.pkl', 'rb') as file:
#     lookup_table = pickle.load(file)
# 
# st.title('Food Image Classifier and Nutrition Calculator')
# 
# # Upload an image
# uploaded_image = st.file_uploader('Upload an image', type=['jpg', 'jpeg', 'png'])
# 
# if uploaded_image is not None:
#     image = Image.open(uploaded_image)
#     st.image(image, caption='Uploaded Image', use_column_width=True)
# 
#     # Preprocess the image for the model
#     image = image.resize((224, 224))
#     image = np.array(image)
#     image = image / 255.0
#     image = np.expand_dims(image, axis=0)
# 
#     # Make a prediction with the model
#     prediction = model.predict(image)
#     predicted_class = np.argmax(prediction)
# 
#     # Get nutritional information from the lookup table
#     food_label = lookup_table[predicted_class]['food_name']
#     calories = lookup_table[predicted_class]['calories']
#     protein = lookup_table[predicted_class]['protein']
#     fat = lookup_table[predicted_class]['fat']
#     carbs = lookup_table[predicted_class]['carbs']
# 
#     st.write(f'Predicted Food: {food_label}')
#     st.write(f'Calories: {calories} kcal')
#     st.write(f'Protein: {protein} g')
#     st.write(f'Fat: {fat} g')
#     st.write(f'Carbohydrates: {carbs} g')
#